<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Charles Martin" />
  <meta name="dcterms.date" content="2021-01-01" />
  <title>Intelligent Musical Instruments Bibliography</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Intelligent Musical Instruments Bibliography</h1>
<p class="author">Charles Martin</p>
<p class="date">2021</p>
</header>
<p>This is a list of suggested references for creating intelligent musical instruments. The references cover machine learning models for creating music and applications in different interactive music scenarios.</p>
<div id="refs" class="references csl-bib-body hanging-indent" data-line-spacing="2" role="doc-bibliography">
<div id="ref-10.1162/ARTL_a_00176" class="csl-entry" role="doc-biblioentry">
Boden, M. A. (2015). <span class="nocase">Creativity and ALife</span>. <em>Artificial Life</em>, <em>21</em>(3), 354–365. <a href="https://doi.org/10.1162/ARTL_a_00176">https://doi.org/10.1162/ARTL_a_00176</a>
</div>
<div id="ref-Briot:2020aa" class="csl-entry" role="doc-biblioentry">
Briot, J.-P., Hadjeres, G., &amp; Pachet, F.-D. (2020). <em>Deep learning techniques for music generation</em>. Springer. <a href="https://doi.org/10.1007/978-3-319-70163-9">https://doi.org/10.1007/978-3-319-70163-9</a>
</div>
<div id="ref-Candy:2014rt" class="csl-entry" role="doc-biblioentry">
Candy, L., &amp; Ferguson, S. (2014). <em>Interactive experience in the digital age: Evaluating new art practice</em> (L. Candy &amp; S. Ferguson, Eds.). Springer International Publishing. <a href="https://doi.org/10.1007/978-3-319-04510-8">https://doi.org/10.1007/978-3-319-04510-8</a>
</div>
<div id="ref-relentlessdoppelganger" class="csl-entry" role="doc-biblioentry">
Carr, C. J., &amp; Zukowski, Z. (2018). <em>Generating albums with SampleRNN to imitate metal, rock, and punk bands</em> (Vol. abs/1811.06633). arXiV Preprint. <a href="http://arxiv.org/abs/1811.06633">http://arxiv.org/abs/1811.06633</a>
</div>
<div id="ref-Donahue:2019aa" class="csl-entry" role="doc-biblioentry">
Donahue, C., Simon, I., &amp; Dieleman, S. (2019). Piano genie. <em>Proceedings of the 24th ACM Conference on Intelligent User Interfaces</em>, 160–164. <a href="https://doi.org/10.1145/3301275.3302288">https://doi.org/10.1145/3301275.3302288</a>
</div>
<div id="ref-Edmonds:2018tr" class="csl-entry" role="doc-biblioentry">
Edmonds, E. (2018). <em>The art of interaction: What HCI can learn from interactive art</em>. Morgan &amp; Claypool Publishers. <a href="https://doi.org/10.2200/S00825ED1V01Y201802HCI039">https://doi.org/10.2200/S00825ED1V01Y201802HCI039</a>
</div>
<div id="ref-Engel:2019aa" class="csl-entry" role="doc-biblioentry">
Engel, J., Agrawal, K. K., Chen, S., Gulrajani, I., Donahue, C., &amp; Roberts, A. (2019). GANSynth: Adversarial neural audio synthesis. <em>Proceedings of ICLR 2019</em>. <a href="https://openreview.net/pdf?id=H1xQVn09FX">https://openreview.net/pdf?id=H1xQVn09FX</a>
</div>
<div id="ref-Fiebrink:2018aa" class="csl-entry" role="doc-biblioentry">
Fiebrink, R., &amp; Gillies, M. (2018). Introduction to the special issue on human-centered machine learning. <em>ACM Trans. Interact. Intell. Syst.</em>, <em>8</em>(2), 7:1–7:7. <a href="https://doi.org/10.1145/3205942">https://doi.org/10.1145/3205942</a>
</div>
<div id="ref-Govalkar:2019aa" class="csl-entry" role="doc-biblioentry">
Govalkar, P., Fischer, J., Zalkow, F., &amp; Dittmar, C. (2019, September). A comparison of recent neural vocoders for speech signal reconstruction. <em>10th <span>ISCA</span> Speech Synthesis Workshop</em>. <a href="https://doi.org/10.21437/ssw.2019-2">https://doi.org/10.21437/ssw.2019-2</a>
</div>
<div id="ref-Ha:2018aa" class="csl-entry" role="doc-biblioentry">
Ha, D., &amp; Schmidhuber, J. (2018). <span>World Models</span>. <em>ArXiv e-Prints</em>. <a href="https://arxiv.org/abs/1803.10122">https://arxiv.org/abs/1803.10122</a>
</div>
<div id="ref-Huang:2018aa" class="csl-entry" role="doc-biblioentry">
Huang, C.-Z. A., Vaswani, A., Uszkoreit, J., Shazeer, N., Simon, I., Hawthorne, C., Dai, A. M., Hoffman, M. D., Dinculescu, M., &amp; Eck, D. (2019). Music transformer: Generating music with long-term structure. <em>Proc. Of ICLR ’19</em>. <a href="https://arxiv.org/abs/1809.04281">https://arxiv.org/abs/1809.04281</a>
</div>
<div id="ref-cybernetics-in-art" class="csl-entry" role="doc-biblioentry">
Jones, S. (2013-01-012013-01-01). Cybernetics in society and art. <em>Proceedings of the 19th International Symposium of Electronic Art</em>. <a href="http://hdl.handle.net/2123/9863">http://hdl.handle.net/2123/9863</a>
</div>
<div id="ref-Lerch:2019aa" class="csl-entry" role="doc-biblioentry">
Lerch, A., Arthur, C., Pati, A., &amp; Gururani, S. (2019). Music performance analysis: A survey. <em>20th International Society for Music Information Retrieval Conference</em>.
</div>
<div id="ref-sound-art" class="csl-entry" role="doc-biblioentry">
Licht, A. (2009). Sound art: Origins, development and ambiguities. <em>Organised Sound</em>, <em>14</em>(1), 3–10. <a href="https://doi.org/10.1017/S1355771809000028">https://doi.org/10.1017/S1355771809000028</a>
</div>
<div id="ref-Luo:2019aa" class="csl-entry" role="doc-biblioentry">
Luo, Y.-J., Agres, K., &amp; Herremans, D. (2019). Learning disentangled representations of timbre and pitch for musical instrument sounds using gaussian mixture variational autoencoders. <em>20th Conference of the International Society for Music Information Retrieval</em>. <a href="http://arxiv.org/abs/1906.08152">http://arxiv.org/abs/1906.08152</a>
</div>
<div id="ref-designing-constraints-music" class="csl-entry" role="doc-biblioentry">
Magnusson, T. (2010). <span class="nocase">Designing Constraints: Composing and Performing with Digital Musical Systems</span>. <em>Computer Music Journal</em>, <em>34</em>(4), 62–73. <a href="https://doi.org/10.1162/COMJ_a_00026">https://doi.org/10.1162/COMJ_a_00026</a>
</div>
<div id="ref-Martin:2017ai" class="csl-entry" role="doc-biblioentry">
Martin, C. P., Ellefsen, K. O., &amp; Torresen, J. (2018). <em>Deep predictive models in interactive music</em>. <a href="http://arxiv.org/abs/1801.10492">http://arxiv.org/abs/1801.10492</a>
</div>
<div id="ref-Martin:2017ae" class="csl-entry" role="doc-biblioentry">
Martin, C. P., Ellefsen, K. O., &amp; Torresen, J. (2017, August). Deep models for ensemble touch-screen improvisation. <em>Proceedings of the 12th International Audio Mostly Conference on Augmented and Participatory Sound and Music Experiences</em>. <a href="https://doi.org/10.1145/3123514.3123556">https://doi.org/10.1145/3123514.3123556</a>
</div>
<div id="ref-empi" class="csl-entry" role="doc-biblioentry">
Martin, C. P., Glette, K., Nygaard, T. F., &amp; Torresen, J. (2020). Understanding musical predictions with an embodied interface for musical machine learning. <em>Frontiers in Artificial Intelligence</em>, <em>3</em>, 6. <a href="https://doi.org/10.3389/frai.2020.00006">https://doi.org/10.3389/frai.2020.00006</a>
</div>
<div id="ref-Martin:2019aa" class="csl-entry" role="doc-biblioentry">
Martin, C. P., &amp; Torresen, J. (2019). An interactive musical prediction system with mixture density recurrent neural networks. In M. Queiroz &amp; A. X. Sedó (Eds.), <em>Proceedings of the international conference on new interfaces for musical expression</em> (pp. 260–265). UFRGS. <a href="http://www.nime.org/proceedings/2019/nime2019_050.pdf">http://www.nime.org/proceedings/2019/nime2019_050.pdf</a>
</div>
<div id="ref-Martin:2018ag" class="csl-entry" role="doc-biblioentry">
Martin, C. P., &amp; Torresen, J. (2018). <span>RoboJam</span>: A musical mixture density network for collaborative touchscreen interaction. In A. Liapis, J. J. Romero Cardalda, &amp; A. Ekárt (Eds.), <em>Computational intelligence in music, sound, art and design: International conference, <span>EvoMUSART</span></em> (Vol. 10783, pp. 161–176). Springer International Publishing. <a href="https://doi.org/10.1007/978-3-319-77583-8_11">https://doi.org/10.1007/978-3-319-77583-8_11</a>
</div>
<div id="ref-microjam" class="csl-entry" role="doc-biblioentry">
Martin, C. P., &amp; Torresen, J. (2020). Data driven analysis of tiny touchscreen performance with MicroJam. <em>Computer Music Journal</em>, <em>43</em>(4), 41–57. <a href="https://doi.org/10.1162/COMJ_a_00536">https://doi.org/10.1162/COMJ_a_00536</a>
</div>
<div id="ref-McCormack:2019aa" class="csl-entry" role="doc-biblioentry">
McCormack, J., Gifford, T., Hutchings, P., Llano Rodriguez, M. T., Yee-King, M., &amp; d’Inverno, M. (2019). In a silent way: Communication between AI and improvising musicians beyond sound. <em>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em>, 38:1–38:11. <a href="https://doi.org/10.1145/3290605.3300268">https://doi.org/10.1145/3290605.3300268</a>
</div>
<div id="ref-Mor:2018aa" class="csl-entry" role="doc-biblioentry">
Mor, N., Wolf, L., Polyak, A., &amp; Taigman, Y. (2018). <span>A Universal Music Translation Network</span>. <em>ArXiv e-Prints</em>. <a href="https://arxiv.org/abs/1805.07848">https://arxiv.org/abs/1805.07848</a>
</div>
<div id="ref-Naess:2019aa" class="csl-entry" role="doc-biblioentry">
Næss, T. R., &amp; Martin, C. P. (2019). A physical intelligent instrument using recurrent neural networks. In M. Queiroz &amp; A. X. Sedó (Eds.), <em>Proceedings of the international conference on new interfaces for musical expression</em> (pp. 79–82). UFRGS. <a href="http://www.nime.org/proceedings/2019/nime2019_016.pdf">http://www.nime.org/proceedings/2019/nime2019_016.pdf</a>
</div>
<div id="ref-rnn-laptop-ensemble" class="csl-entry" role="doc-biblioentry">
Proctor, R., &amp; Martin, C. P. (2020). A laptop ensemble performance system using recurrent neural networks. In R. Michon &amp; F. Schroeder (Eds.), <em>Proceedings of the international conference on new interfaces for musical expression</em> (pp. 43–48). Birmingham City University. <a href="https://www.nime.org/proceedings/2020/nime2020_paper9.pdf">https://www.nime.org/proceedings/2020/nime2020_paper9.pdf</a>
</div>
<div id="ref-Roberts:2019aa" class="csl-entry" role="doc-biblioentry">
Roberts, A., Engel, J., Mann, Y., Gillick, J., Kayacik, C., Nørly, S., Dinculescu, M., Radebaugh, C., Hawthorne, C., &amp; Eck, D. (2019). Magenta studio: Augmenting creativity with deep learning in ableton live. <em>Proceedings of the International Workshop on Musical Metacreation (MUME)</em>. <a href="http://musicalmetacreation.org/buddydrive/file/mume_2019_paper_2/">http://musicalmetacreation.org/buddydrive/file/mume_2019_paper_2/</a>
</div>
<div id="ref-Roberts:2018aa" class="csl-entry" role="doc-biblioentry">
Roberts, A., Engel, J., Raffel, C., Hawthorne, C., &amp; Eck, D. (2018). <span class="nocase">A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music</span>. <em>ArXiv e-Prints</em>. <a href="https://arxiv.org/abs/1803.05428">https://arxiv.org/abs/1803.05428</a>
</div>
<div id="ref-10.1145/3450741.3465245" class="csl-entry" role="doc-biblioentry">
Wallace, B., Martin, C. P., Tørresen, J., &amp; Nymoen, K. (2021). Learning embodied sound-motion mappings: Evaluating AI-generated dance improvisation. <em>Creativity and Cognition</em>. <a href="https://doi.org/10.1145/3450741.3465245">https://doi.org/10.1145/3450741.3465245</a>
</div>
<div id="ref-Yang:2018aa" class="csl-entry" role="doc-biblioentry">
Yang, L.-C., &amp; Lerch, A. (2018). On the evaluation of generative models in music. <em>Neural Computing and Applications</em>. <a href="https://doi.org/10.1007/s00521-018-3849-7">https://doi.org/10.1007/s00521-018-3849-7</a>
</div>
</div>
</body>
</html>
